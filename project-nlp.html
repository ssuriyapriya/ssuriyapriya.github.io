<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NLP Text Summarizer | Suriyapriya S.</title>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;500;600;700;800;900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
    <style>
        /* [Reuse Same Styles from Diabetes File] */
        body { font-family: 'Montserrat', sans-serif; margin: 0; padding: 0; color: #546e7a; background-color: #fafafa; line-height: 1.8; }
        .project-header { background-color: #fce4ec; padding: 100px 20px; text-align: center; border-bottom: 6px solid #ec407a; }
        .project-title { font-size: 3rem; font-weight: 900; color: #37474f; margin: 0; letter-spacing: -1px; text-transform: uppercase; }
        .project-subtitle { font-size: 1.3rem; font-weight: 600; color: #ec407a; margin-top: 20px; text-transform: uppercase; letter-spacing: 1px; }
        .header-meta { margin-top: 30px; font-size: 1rem; font-weight: 500; color: #78909c; }
        .nav-bar { background-color: white; box-shadow: 0 2px 10px rgba(0,0,0,0.05); padding: 20px 40px; position: sticky; top: 0; z-index: 1000; display: flex; justify-content: space-between; align-items: center; }
        .nav-brand { font-weight: 800; font-size: 1.2rem; color: #ec407a; text-decoration: none; }
        .nav-links a { color: #37474f; text-decoration: none; margin-left: 30px; font-weight: 600; font-size: 0.9rem; transition: 0.3s; }
        .nav-links a:hover { color: #ec407a; }
        .container { max-width: 1100px; margin: 0 auto; padding: 60px 20px; }
        .section { display: flex; flex-wrap: wrap; align-items: center; margin-bottom: 100px; gap: 60px; }
        .section:nth-child(even) { flex-direction: row-reverse; }
        .text-col { flex: 1; min-width: 300px; }
        .img-col { flex: 1; min-width: 300px; }
        h2 { font-size: 2rem; font-weight: 800; color: #ec407a; margin-bottom: 25px; position: relative; display: inline-block; text-transform: uppercase; }
        h3 { font-size: 1.3rem; font-weight: 700; color: #37474f; margin-top: 30px; margin-bottom: 15px; }
        p { margin-bottom: 20px; text-align: justify; font-size: 1.05rem; }
        ul { list-style: none; padding: 0; }
        li { margin-bottom: 15px; padding-left: 25px; position: relative; font-size: 1.05rem; }
        li::before { content: "â€¢"; color: #ec407a; font-weight: bold; font-size: 1.5rem; position: absolute; left: 0; top: -5px; }
        .img-placeholder { width: 100%; border-radius: 12px; box-shadow: 0 15px 40px rgba(236, 64, 122, 0.15); transition: transform 0.3s; border: 4px solid white; }
        .img-placeholder:hover { transform: translateY(-5px); }
        .caption { text-align: center; font-size: 0.85rem; color: #90a4ae; margin-top: 15px; font-weight: 500; }
        .highlight-box { background-color: white; border-left: 5px solid #ec407a; padding: 25px; border-radius: 8px; margin: 30px 0; box-shadow: 0 5px 20px rgba(0,0,0,0.05); font-weight: 500; color: #37474f; }
        
        table { width: 100%; border-collapse: collapse; margin: 30px 0; background: white; border-radius: 8px; overflow: hidden; box-shadow: 0 5px 15px rgba(0,0,0,0.05); }
        th { background-color: #ec407a; color: white; padding: 15px; text-align: left; font-weight: 700; }
        td { padding: 15px; border-bottom: 1px solid #eee; color: #546e7a; }
        tr:last-child td { border-bottom: none; }

        /* Footer */
        .footer { background-color: #37474f; color: white; text-align: center; padding: 80px 20px; margin-top: 100px; display: flex; flex-direction: column; align-items: center; justify-content: center; }
        .footer h2 { color: white !important; margin-bottom: 10px; text-align: center; }
        .footer p { color: #cfd8dc; text-align: center !important; margin: 0 auto 30px auto; }
        .btn-back { display: inline-block; padding: 15px 40px; background-color: #ec407a; color: white; text-decoration: none; border-radius: 50px; font-weight: 800; text-transform: uppercase; letter-spacing: 1px; transition: 0.3s; box-shadow: 0 5px 15px rgba(0,0,0,0.3); }
        .btn-back:hover { background-color: white; color: #ec407a; }
    </style>
</head>
<body>
    <nav class="nav-bar">
        <a href="index.html" class="nav-brand">Suriyapriya S.</a>
        <div class="nav-links"><a href="index.html">Portfolio Home</a><a href="#pipeline">Methodology</a><a href="#results">Results</a></div>
    </nav>
    <header class="project-header">
        <div class="container" style="padding:0;">
            <h1 class="project-title">NLP Based Text Summarizer</h1>
            <div class="project-subtitle">Automated Information Extraction from Medical Reports</div>
            <div class="header-meta"><span style="margin: 0 15px;"><i class="fa fa-calendar"></i> April 2023</span><span style="margin: 0 15px;"><i class="fa fa-code"></i> Python, OpenCV, Pytesseract, EAST</span><span style="margin: 0 15px;"><i class="fa fa-users"></i> Role: Algorithm Developer</span></div>
        </div>
    </header>
    <div class="container">
        
        <div class="section" id="intro">
            <div class="text-col">
                <h2>The Challenge: Unstructured Data</h2>
                <p>Medical errors due to illegible handwriting cause an estimated <strong>7,000 preventable deaths annually</strong> in the US alone. Existing systems struggle to process mixed formats (handwritten + printed) in medical reports, leading to inefficiencies and data loss.</p>
                <div class="highlight-box"><strong>Project Goal:</strong> "To develop a system that interprets handwritten medical notes into a digital form, converting unstructured data into a structured, searchable, and easily readable format."</div>
            </div>
            <div class="img-col">
                <img src="assets/img/output.png" class="img-placeholder" alt="Final Digital Output">
                <div class="caption">Figure 1: Project Output - Converting raw medical data into digital text.</div>
            </div>
        </div>
        
        <div class="section" id="pipeline">
            <div class="img-col">
                <img src="assets/img/method.png" class="img-placeholder" alt="Methodology Flowchart">
                <div class="caption">Figure 2: System Architecture & Data Flow.</div>
            </div>
            <div class="text-col">
                <h2>System Architecture & Methodology</h2>
                <p>The solution utilizes a multi-stage pipeline combining Computer Vision for preprocessing and Deep Learning for recognition.</p>
                <ul>
                    <li><strong>Adaptive Thresholding:</strong> Used OpenCV to convert grayscale images into binary format, separating text from the background using Otsu's method.</li>
                    <li><strong>Connected Component Analysis (CCA):</strong> Algorithms were used to identify and label groups of pixels to segment text regions.</li>
                    <li><strong>EAST Text Detector:</strong> Implemented the Efficient and Accurate Scene Text (EAST) detector to localize word regions even in noisy backgrounds.</li>
                </ul>
            </div>
        </div>

        <div class="section">
             <div class="text-col">
                <h2>Medical Entity Recognition</h2>
                <p>Beyond simple OCR, the system needed to understand context. We implemented a secondary layer to interpret specific medical terminology, linking abbreviated handwritten notes to standardized medical definitions.</p>
            </div>
            <div class="img-col">
                <img src="assets/img/interpret-med-terms.png" class="img-placeholder" alt="Interpreting Medical Terms">
                <div class="caption">Figure 3: Logic for Interpreting and Standardizing Medical Terms.</div>
            </div>
        </div>

        <div class="section" id="results">
            <div class="text-col">
                <h2>Performance Evaluation</h2>
                <p>The model was tested on a dataset of 772 handwritten cursive images and printed reports. We measured accuracy at both the character and word levels.</p>
                <table>
                    <tr><th>Text Type</th><th>Character Accuracy</th><th>Word Accuracy</th></tr>
                    <tr><td>Printed Text</td><td>99.04%</td><td>94.79%</td></tr>
                    <tr><td>Handwritten Cursive</td><td>84.75%</td><td>65.74%</td></tr>
                    <tr><td><strong>Overall System</strong></td><td><strong>91.26%</strong></td><td><strong>-</strong></td></tr>
                </table>
                <p>Our proposed method (91.26%) outperformed standard baselines like Wei et al. (90.6%), demonstrating the robustness of the hybrid approach.</p>
            </div>
            <div class="img-col">
                <img src="assets/img/ocr-accuracy.png" class="img-placeholder" alt="OCR Accuracy Chart">
                <div class="caption">Figure 4: OCR Accuracy Comparison (Character vs. Word Level).</div>
            </div>
        </div>
    </div>
    <div class="footer"><h2>Ready to see more?</h2><p>Explore my other projects or get in touch.</p><a href="index.html" class="btn-back">Back to Portfolio</a></div>
</body>
</html>
